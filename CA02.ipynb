{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0grl9b64yaq",
        "outputId": "0b5e337d-f306-471f-c233-06719c8c3a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Machine Learning imports from scikit-learn\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "nxG6tGal5KCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_Dictionary(root_dir):\n",
        "    \"\"\"\n",
        "    Creates a dictionary of the 3000 most frequent valid words from training emails.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    all_words = []  # List to store all words from all emails\n",
        "\n",
        "    # Create list of full file paths for all files in the directory\n",
        "    emails = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
        "\n",
        "    # Iterate through each email file\n",
        "    for mail in emails:\n",
        "        with open(mail) as m:  # Open each email file\n",
        "            for line in m:  # Read line by line\n",
        "                words = line.split()  # Split line into words (whitespace delimiter)\n",
        "                all_words += words  # Add words to our master list\n",
        "\n",
        "    # Count frequency of each word using Counter (dictionary subclass)\n",
        "    dictionary = Counter(all_words)\n",
        "\n",
        "    # Create a copy of all words for safe iteration during deletion\n",
        "    list_to_remove = list(dictionary)\n",
        "\n",
        "    # Clean the dictionary by removing unwanted items\n",
        "    for item in list_to_remove:\n",
        "        # Remove words containing non-alphabetic characters (numbers, punctuation)\n",
        "        if item.isalpha() == False:\n",
        "            del dictionary[item]\n",
        "        # Remove single-character words (usually not meaningful)\n",
        "        elif len(item) == 1:\n",
        "            del dictionary[item]\n",
        "\n",
        "    # Get the 3000 most common words as a list of (word, count) tuples\n",
        "    dictionary = dictionary.most_common(3000)\n",
        "\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "AiDiw8GnqNtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a dictionary function to reduces dimensionality (instead of thousands of unique words, we use only 3000), remove noise, and create a consistent feature space for the machine learning model.\n",
        "\n",
        "\n",
        "Process:\n",
        "1. Read all email files from the training directory\n",
        "2. Extract all the words\n",
        "3. Use Counter to cound the word frequencies\n",
        "4. Remove non-alphabetic words (numbers, punctuation, symbols)\n",
        "5. Remove single-character words (usually not meaningful)\n",
        "6. Return the 3000 most common words"
      ],
      "metadata": {
        "id": "j6IDyBNUruOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(mail_dir):\n",
        "    \"\"\"\n",
        "    Extracts features from emails and creates a numerical feature matrix.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Get files\n",
        "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
        "\n",
        "    # Initialize feature matrix\n",
        "    features_matrix = np.zeros((len(files), 3000))\n",
        "\n",
        "    # Initialize labels array: 0 = non-spam, 1 = spam\n",
        "    train_labels = np.zeros(len(files))\n",
        "\n",
        "    count = 1\n",
        "    docID = 0\n",
        "\n",
        "    # Process each email\n",
        "    for fil in files:\n",
        "        with open(fil) as fi:\n",
        "            for i, line in enumerate(fi):\n",
        "                if i == 2:\n",
        "                    words = line.split()  # Split content into words\n",
        "\n",
        "                    # For each word in email\n",
        "                    for word in words:\n",
        "                        wordID = 0\n",
        "\n",
        "                        # Search for this word in our dictionary\n",
        "                        for i, d in enumerate(dictionary):\n",
        "                            if d[0] == word:\n",
        "                                wordID = i\n",
        "                                # Count occurrences and store in feature matrix\n",
        "                                features_matrix[docID, wordID] = words.count(word)\n",
        "\n",
        "        # Default label is 0\n",
        "        train_labels[docID] = 0\n",
        "\n",
        "        # Parse filename to determine if email is spam\n",
        "        filepathTokens = fil.split('/')\n",
        "        lastToken = filepathTokens[len(filepathTokens) - 1]\n",
        "\n",
        "        # If filename starts with \"spmsg\", it's a spam email\n",
        "        if lastToken.startswith(\"spmsg\"):\n",
        "            train_labels[docID] = 1\n",
        "            count = count + 1\n",
        "\n",
        "        docID = docID + 1  # Move to next email\n",
        "\n",
        "    return features_matrix, train_labels"
      ],
      "metadata": {
        "id": "LPqzMWv1qVvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function converts emails into a **numerical feature matrix** that can be used by machine learning algorithms.\n",
        "\n",
        "Process:\n",
        "1. Read all email files from directory\n",
        "2. For each email:\n",
        "   - Extract words from line 3 (actual content, skipping subject and blank line)\n",
        "   - Count how many times each dictionary word appears\n",
        "   - Store counts in the feature matrix\n",
        "   - Label the email as spam (1) or non-spam (0) based on filename\n",
        "3. Return the feature matrix and labels\n",
        "\n",
        "Example:\n",
        "If word \"free\" is at index 42 in dictionary and appears 3 times in email 5:\n",
        "- `features_matrix[5, 42] = 3`"
      ],
      "metadata": {
        "id": "uRAQTcVWr1qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define relative paths to data directories\n",
        "TRAIN_DIR = './train-mails'  # Training data: 702 emails\n",
        "TEST_DIR = './test-mails'    # Test data: 260 emails"
      ],
      "metadata": {
        "id": "C15mDZjrqYy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Build the dictionary from training emails\n",
        "# This analyzes all training data and selects the 3000 most frequent valid words\n",
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
        "\n",
        "# Step 2: Extract features from training data\n",
        "# Converts training emails to numerical matrix and extracts labels\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "\n",
        "# Step 3: Extract features from test data\n",
        "# Uses the SAME dictionary fore consistency\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0FBjMBuqcCg",
        "outputId": "41a59d79-929c-48a0-ce41-2295826cf360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps explained:\n",
        "1. Create Dictionary:\n",
        "   - Analyze all training emails\n",
        "   - Extract the 3000 most common valid words\n",
        "   - This becomes our feature set (vocabulary)\n",
        "\n",
        "2. Extract Training Features:\n",
        "   - Convert 702 training emails into a 702×3000 matrix\n",
        "   - Each cell contains word frequency\n",
        "   - Create corresponding labels (spam vs. non-spam)\n",
        "\n",
        "3. Extract Test Features:\n",
        "   - Convert 260 test emails into a 260×3000 matrix\n",
        "   - Use the SAME dictionary created from training data\n",
        "   - Create corresponding labels for evaluation\n",
        "  \n"
      ],
      "metadata": {
        "id": "3JUG4uyksAL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING PHASE\n",
        "print(\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "\n",
        "# Initialize the Gaussian Naive Bayes classifier\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model on training features and labels\n",
        "model.fit(features_matrix, labels)\n",
        "\n",
        "print(\"Training completed\")\n",
        "\n",
        "# PREDICTION PHASE\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "\n",
        "# Use the trained model to predict labels for test emails\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "\n",
        "# EVALUATION PHASE\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "\n",
        "# Calculate accuracy: (number of correct predictions) / (total predictions)\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11lQ4GzKqnjb",
        "outputId": "c398c590-ee01-4e02-9114-ef330dd25ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model using Gaussian Naive Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy Score: 0.9654 (96.54%)\n",
        "\n",
        "The model demonstrates strong performance, correctly classifying approximately 251 out of 260 test emails and misclassifying only about 9. This level of accuracy suggests that the Naive Bayes approach is effective for spam detection, and that a 3000-word feature space captures enough information to distinguish between spam and legitimate emails. Overall, the results highlight word frequency as a reliable signal for separating the two classes.\n",
        "\n",
        "\n",
        "Potential Improvements:\n",
        "1. Use TF-IDF instead of raw word counts\n",
        "2. Include bigrams or trigrams (word pairs/triplets)\n",
        "3. Try other algorithms (SVM, Random Forest, Neural Networks)\n",
        "4. Perform cross-validation for more robust evaluation\n",
        "5. Add subject line as a separate feature\n"
      ],
      "metadata": {
        "id": "2qZ9PrJEsPhX"
      }
    }
  ]
}